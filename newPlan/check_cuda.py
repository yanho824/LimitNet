#!/usr/bin/env python3
"""
CUDAè¯Šæ–­è„šæœ¬
æ£€æŸ¥CUDAå¯ç”¨æ€§å’Œç›¸å…³é…ç½®
"""

import sys
import os
import subprocess
import platform


def print_section(title):
    """æ‰“å°æ ‡é¢˜"""
    print(f"\n{'='*60}")
    print(f"ğŸ” {title}")
    print(f"{'='*60}")


def run_command(command):
    """è¿è¡Œå‘½ä»¤å¹¶è¿”å›ç»“æœ"""
    try:
        result = subprocess.run(command, shell=True, capture_output=True, text=True)
        return result.stdout.strip(), result.returncode == 0
    except Exception as e:
        return str(e), False


def check_system_info():
    """æ£€æŸ¥ç³»ç»Ÿä¿¡æ¯"""
    print_section("ç³»ç»Ÿä¿¡æ¯")
    
    print(f"æ“ä½œç³»ç»Ÿ: {platform.system()} {platform.release()}")
    print(f"æ¶æ„: {platform.machine()}")
    print(f"Pythonç‰ˆæœ¬: {sys.version}")
    print(f"Pythonè·¯å¾„: {sys.executable}")


def check_nvidia_driver():
    """æ£€æŸ¥NVIDIAé©±åŠ¨"""
    print_section("NVIDIAé©±åŠ¨æ£€æŸ¥")
    
    output, success = run_command("nvidia-smi")
    if success:
        print("âœ… NVIDIAé©±åŠ¨å·²å®‰è£…")
        print(output)
        return True
    else:
        print("âŒ NVIDIAé©±åŠ¨æœªå®‰è£…æˆ–ä¸å¯ç”¨")
        print("è¯·å®‰è£…NVIDIAæ˜¾å¡é©±åŠ¨ç¨‹åº")
        return False


def check_cuda_toolkit():
    """æ£€æŸ¥CUDAå·¥å…·åŒ…"""
    print_section("CUDAå·¥å…·åŒ…æ£€æŸ¥")
    
    # æ£€æŸ¥nvccå‘½ä»¤
    output, success = run_command("nvcc --version")
    if success:
        print("âœ… CUDAå·¥å…·åŒ…å·²å®‰è£…")
        print(output)
        return True
    else:
        print("âŒ CUDAå·¥å…·åŒ…æœªå®‰è£…æˆ–ä¸åœ¨PATHä¸­")
        
        # æ£€æŸ¥å¸¸è§çš„CUDAå®‰è£…è·¯å¾„
        cuda_paths = [
            "C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA",
            "/usr/local/cuda",
            "/opt/cuda"
        ]
        
        for path in cuda_paths:
            if os.path.exists(path):
                print(f"å‘ç°CUDAå®‰è£…è·¯å¾„: {path}")
                break
        else:
            print("æœªæ‰¾åˆ°CUDAå®‰è£…è·¯å¾„")
        return False


def check_pytorch():
    """æ£€æŸ¥PyTorchå®‰è£…"""
    print_section("PyTorchæ£€æŸ¥")
    
    try:
        import torch
        print(f"âœ… PyTorchå·²å®‰è£…ï¼Œç‰ˆæœ¬: {torch.__version__}")
        
        # æ£€æŸ¥CUDAæ”¯æŒ
        cuda_available = torch.cuda.is_available()
        print(f"CUDAå¯ç”¨: {'âœ… æ˜¯' if cuda_available else 'âŒ å¦'}")
        
        if cuda_available:
            print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
            print(f"cuDNNç‰ˆæœ¬: {torch.backends.cudnn.version()}")
            print(f"GPUæ•°é‡: {torch.cuda.device_count()}")
            
            for i in range(torch.cuda.device_count()):
                gpu_name = torch.cuda.get_device_name(i)
                gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
                print(f"GPU {i}: {gpu_name} ({gpu_memory:.1f}GB)")
        else:
            print("\nâŒ PyTorchæ£€æµ‹ä¸åˆ°CUDAæ”¯æŒ")
            print("å¯èƒ½çš„åŸå› :")
            print("1. å®‰è£…äº†CPUç‰ˆæœ¬çš„PyTorch")
            print("2. CUDAç‰ˆæœ¬ä¸åŒ¹é…")
            print("3. NVIDIAé©±åŠ¨ç¨‹åºé—®é¢˜")
            
        return cuda_available
        
    except ImportError:
        print("âŒ PyTorchæœªå®‰è£…")
        return False


def check_environment():
    """æ£€æŸ¥ç¯å¢ƒå˜é‡"""
    print_section("ç¯å¢ƒå˜é‡æ£€æŸ¥")
    
    env_vars = ['CUDA_HOME', 'CUDA_PATH', 'CUDA_ROOT', 'PATH', 'LD_LIBRARY_PATH']
    
    for var in env_vars:
        value = os.environ.get(var)
        if value:
            print(f"{var}: {value}")
        else:
            print(f"{var}: æœªè®¾ç½®")


def provide_solutions():
    """æä¾›è§£å†³æ–¹æ¡ˆ"""
    print_section("è§£å†³æ–¹æ¡ˆå»ºè®®")
    
    print("ğŸ”§ å¦‚æœCUDAä¸å¯ç”¨ï¼Œè¯·å°è¯•ä»¥ä¸‹è§£å†³æ–¹æ¡ˆ:")
    print()
    
    print("1ï¸âƒ£ å®‰è£…NVIDIAé©±åŠ¨ç¨‹åº:")
    print("   - è®¿é—® https://www.nvidia.com/drivers")
    print("   - ä¸‹è½½å¹¶å®‰è£…é€‚åˆæ‚¨æ˜¾å¡çš„æœ€æ–°é©±åŠ¨")
    print()
    
    print("2ï¸âƒ£ å®‰è£…CUDAå·¥å…·åŒ…:")
    print("   - è®¿é—® https://developer.nvidia.com/cuda-downloads")
    print("   - ä¸‹è½½å¹¶å®‰è£…CUDAå·¥å…·åŒ…ï¼ˆæ¨è11.8æˆ–12.xç‰ˆæœ¬ï¼‰")
    print()
    
    print("3ï¸âƒ£ é‡æ–°å®‰è£…PyTorchï¼ˆCUDAç‰ˆæœ¬ï¼‰:")
    print("   # å¸è½½å½“å‰PyTorch")
    print("   pip uninstall torch torchvision torchaudio")
    print()
    print("   # å®‰è£…CUDAç‰ˆæœ¬çš„PyTorch")
    print("   # CUDA 11.8:")
    print("   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118")
    print()
    print("   # CUDA 12.1:")
    print("   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121")
    print()
    
    print("4ï¸âƒ£ éªŒè¯å®‰è£…:")
    print("   python -c \"import torch; print(f'CUDAå¯ç”¨: {torch.cuda.is_available()}')\"")
    print()
    
    print("5ï¸âƒ£ å¦‚æœä»ç„¶ä¸å·¥ä½œ:")
    print("   - é‡å¯è®¡ç®—æœº")
    print("   - æ£€æŸ¥Windowsæ›´æ–°")
    print("   - ç¡®ä¿æ²¡æœ‰å†²çªçš„æ˜¾å¡é©±åŠ¨")
    print("   - åœ¨è®¾å¤‡ç®¡ç†å™¨ä¸­æ£€æŸ¥æ˜¾å¡çŠ¶æ€")


def test_cuda_operations():
    """æµ‹è¯•CUDAæ“ä½œ"""
    print_section("CUDAåŠŸèƒ½æµ‹è¯•")
    
    try:
        import torch
        
        if torch.cuda.is_available():
            print("ğŸ§ª æµ‹è¯•CUDAæ“ä½œ...")
            
            # åˆ›å»ºå¼ é‡
            device = torch.device('cuda')
            x = torch.randn(1000, 1000, device=device)
            y = torch.randn(1000, 1000, device=device)
            
            # çŸ©é˜µä¹˜æ³•æµ‹è¯•
            import time
            start_time = time.time()
            z = torch.mm(x, y)
            end_time = time.time()
            
            print(f"âœ… CUDAçŸ©é˜µä¹˜æ³•æµ‹è¯•æˆåŠŸ")
            print(f"   è®¡ç®—æ—¶é—´: {end_time - start_time:.4f}ç§’")
            print(f"   ç»“æœå½¢çŠ¶: {z.shape}")
            print(f"   GPUå†…å­˜ä½¿ç”¨: {torch.cuda.memory_allocated() / 1024**2:.1f}MB")
            
            # æ¸…ç†GPUå†…å­˜
            del x, y, z
            torch.cuda.empty_cache()
            
        else:
            print("âŒ CUDAä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡Œæµ‹è¯•")
            
    except Exception as e:
        print(f"âŒ CUDAæµ‹è¯•å¤±è´¥: {e}")


def main():
    print("ğŸš€ CUDAè¯Šæ–­å·¥å…·")
    print("è¿™ä¸ªå·¥å…·å°†å¸®åŠ©æ‚¨è¯Šæ–­CUDAé…ç½®é—®é¢˜")
    
    # ç³»ç»Ÿä¿¡æ¯
    check_system_info()
    
    # æ£€æŸ¥NVIDIAé©±åŠ¨
    driver_ok = check_nvidia_driver()
    
    # æ£€æŸ¥CUDAå·¥å…·åŒ…
    cuda_toolkit_ok = check_cuda_toolkit()
    
    # æ£€æŸ¥PyTorch
    pytorch_cuda_ok = check_pytorch()
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    check_environment()
    
    # å¦‚æœCUDAå¯ç”¨ï¼Œè¿›è¡ŒåŠŸèƒ½æµ‹è¯•
    if pytorch_cuda_ok:
        test_cuda_operations()
    
    # æä¾›è§£å†³æ–¹æ¡ˆ
    if not pytorch_cuda_ok:
        provide_solutions()
    
    print_section("è¯Šæ–­å®Œæˆ")
    
    if pytorch_cuda_ok:
        print("ğŸ‰ CUDAé…ç½®æ­£å¸¸ï¼æ‚¨å¯ä»¥ä½¿ç”¨GPUåŠ é€Ÿè®­ç»ƒã€‚")
    else:
        print("âš ï¸  CUDAé…ç½®æœ‰é—®é¢˜ï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒï¼ˆé€Ÿåº¦è¾ƒæ…¢ï¼‰ã€‚")
        print("å»ºè®®æŒ‰ç…§ä¸Šè¿°è§£å†³æ–¹æ¡ˆä¿®å¤CUDAé…ç½®ã€‚")


if __name__ == "__main__":
    main() 